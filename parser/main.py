import asyncio 
import pandas as pd
import os
import json
from pathlib import Path
from parser_23MET import ParserSite_23MET
from proxyParser import ParserProxyLib
from preProcessor import PreProcessor
from update_config import change_update_config_json

async def main(with_proxy=False):
    print("–°–∫—Ä–∏–ø—Ç –∑–∞–ø—É—â–µ–Ω!")
    script_dir = Path(__file__).parent.resolve()
    os.chdir(script_dir)
    
    change_update_config_json(os.path.join(os.getcwd(), 'config.json'))

    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–ø–∏—Å–æ–∫ —Å–∞–π—Ç–æ–≤ –∏–∑ ALL_HREFS.json
    all_hrefs_file = os.path.join(os.getcwd(), 'GoogleHTML', 'ALL_HREFS.json')
    if not os.path.exists(all_hrefs_file):
        print(f"‚ùå –§–∞–π–ª {all_hrefs_file} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return
    
    with open(all_hrefs_file, 'r', encoding='utf-8') as f:
        urls = json.load(f)
    
    print(f"üìã –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(urls)} —Å–∞–π—Ç–æ–≤ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞")

    if with_proxy:
        proxy = ParserProxyLib(max_rate=100, time_period=1)
        await proxy.parsing(url_for_checking='https://23met.ru/')
        proxy_list = proxy.get_sockets()
        if not proxy_list:
            proxy_list = None
        main_parser = ParserSite_23MET(max_rate=100, proxy_list=proxy_list)
    else:
        # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ - –∏—â–µ–º —Ç–æ–ª—å–∫–æ —Ç—Ä—É–±—ã –í–ì–ü
        filter_keywords = ["–¢—Ä—É–±–∞ –í–ì–ü", "–¢—Ä—É–±–∞ –±/—à –≥/–¥", "–¢—Ä—É–±–∞ —ç/—Å"]
        main_parser = ParserSite_23MET(max_rate=100, 
                                      filter_keywords=filter_keywords, 
                                      filter_mode="any")
    
    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–ø–∏—Å–æ–∫ —Å–∞–π—Ç–æ–≤ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
    main_parser.set_urls(urls)
    
    df = await main_parser.run(with_update_sites_info=False,
                              with_save_result=True,
                              with_remove_intermediate_data=False)

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
    result_file = os.path.join(os.getcwd(), '23MET_DATA', 'result.csv')
    if os.path.exists(result_file) and os.path.getsize(result_file) > 0:
        print("–ù–∞—á–∏–Ω–∞—é –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É –¥–∞–Ω–Ω—ã—Ö")
        preprocessor = PreProcessor(csv_file_path=result_file)
        
        preprocessing_file = os.path.join(os.getcwd(), '23MET_DATA', 'preprocessing_result.csv')
        print("–ó–∞–∫–æ–Ω—á–∏–ª—Å—è –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥. –Ω–∞—á–∏–Ω–∞—é —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø–æ –ø—É—Ç–∏:", preprocessing_file)
        preprocessor.save_data(path=preprocessing_file)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        processed_df = pd.read_csv(preprocessing_file, index_col=0)
        
        print(f"\nüéØ –ò—Ç–æ–≥–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:")
        print(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π: {len(processed_df)}")
        print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–æ–º–ø–∞–Ω–∏–π: {processed_df['–ö–æ–º–ø–∞–Ω–∏—è'].nunique()}")
        print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤: {processed_df['–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ'].nunique()}")
        
        print(f"\nüíæ –§–∞–π–ª—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:")
        print(f"   ‚Ä¢ preprocessing_result.csv - –ø–æ–ª–Ω—ã–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
    else:
        print("‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ - —Ñ–∞–π–ª result.csv –ø—É—Å—Ç –∏–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
    
    print('–†–∞–±–æ—Ç–∞ –∑–∞–∫–æ–Ω—á–∏–ª–∞—Å—å')

if __name__ == "__main__":
    asyncio.run(main())
