import aiohttp
import asyncio
import aiolimiter
from bs4 import BeautifulSoup

import os
import pandas as pd
from typing import Union

from parser.GoogleParser import GoogleParser
from parser.base import Parser


class ParserSite_23MET(Parser):
    def __init__(self, 
                 base_url: str= "https://23met.ru",
                 proxy_list: list= None,
                 max_rate: int= 1,
                 time_period: int= 10,
                 filter_keywords: list= None,
                 filter_mode: str= "any"):
        """
        Args:
            base_url (str, optional): –¥–æ–º–µ–Ω–Ω–æ–µ –∏–º—è —Å–∞–π—Ç–∞. Defaults to "https://23met.ru".
            proxy_list (list, optional): –°–ø–∏—Å–æ–∫ –ø—Ä–æ–∫—Å–∏ –∞–¥—Ä–µ—Å–æ–≤. Defaults to None.
            max_rate (int, optional): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—Ä–æ—Å–æ–≤ –∑–∞ time_period. Defaults to 1.
            time_period (int, optional): –í—Ä–µ–º—è –∑–∞ –∫–æ—Ç–æ—Ä–æ–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è max_rate –∑–∞–ø—Ä–æ—Å–æ–≤. Defaults to 10.
            filter_keywords (list, optional): –°–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏. Defaults to None.
            filter_mode (str, optional): –†–µ–∂–∏–º —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: "any" (–ª—é–±–æ–µ —Å–ª–æ–≤–æ) –∏–ª–∏ "all" (–≤—Å–µ —Å–ª–æ–≤–∞). Defaults to "any".
        """

        super().__init__(base_url, proxy_list)
        self.__limiter = aiolimiter.AsyncLimiter(max_rate= max_rate,
                                                 time_period= time_period)
        self.__filter_keywords = filter_keywords or []
        self.__filter_mode = filter_mode
        DIR_NAME = "23MET_DATA"
        os.makedirs(DIR_NAME, exist_ok=True)
        self._dir_path = os.path.join(os.getcwd(), DIR_NAME) 
        self.__file_paths = None
        self.__unique_columns_name = None
        self.__custom_urls = None

    def set_urls(self, urls: list):
        """
        –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç —Å–ø–∏—Å–æ–∫ URL –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤–º–µ—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Google –ø–æ–∏—Å–∫–∞
        Args:
            urls (list): –°–ø–∏—Å–æ–∫ URL –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
        """
        self.__custom_urls = urls
        print(f"‚úÖ –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω —Å–ø–∏—Å–æ–∫ –∏–∑ {len(urls)} —Å–∞–π—Ç–æ–≤ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞")

    async def __get_and_save_site_data(self, 
                                       session: aiohttp.ClientSession, 
                                       url:str, 
                                       accept: str) -> None:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ html —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ –µ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ñ–∞–π–ª  
        Args:
            session (aiohttp.ClientSession): –°–µ—Å—Å–∏—è
            url (str): url 
            accept (str, optional): —Ç–∏–ø—ã —Ñ–∞–π–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –∫–ª–∏–µ–Ω—Ç –º–æ–∂–µ—Ç –ø—Ä–∏–Ω—è—Ç—å (–æ—Ç–æ–±—Ä–∞–∂–∞–µ—Ç—Å—è –±—Ä–∞—É–∑–µ—Ä–æ–º –≤ header-e). Defaults to '*/*'.
        Returns:
            None
        """
        data = await self.get_html(session= session,
                                   url= url,
                                   accept= accept)
        if self.__checking(data):
            file_path = os.path.join(self._dir_path, url.split('/')[-1] + ".html")
            await self.put_file(path= file_path, data= data)
    
    async def __process_single_url_with_limiter(self,
                                                session: aiohttp.ClientSession, 
                                                url:str, 
                                                accept: str) -> None:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ html —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ –µ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ñ–∞–π–ª, –Ω–æ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –∑–∞–ø—Ä–æ—Å–æ–≤ –∑–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–µ –≤—Ä–µ–º—è. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–ª-–≤–∞ –∑–∞–ø—Ä–æ—Å–æ–≤ –∑–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–µ –≤—Ä–µ–º—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç—Å—è –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–∞, —Å –ø–æ–º–æ—â—å—é –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ max_rate –∏ time_period.
        Args:
            session (aiohttp.ClientSession): –°–µ—Å—Å–∏—è
            url (str): url 
            accept (str, optional): —Ç–∏–ø—ã —Ñ–∞–π–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –∫–ª–∏–µ–Ω—Ç –º–æ–∂–µ—Ç –ø—Ä–∏–Ω—è—Ç—å (–æ—Ç–æ–±—Ä–∞–∂–∞–µ—Ç—Å—è –±—Ä–∞—É–∑–µ—Ä–æ–º –≤ header-e). Defaults to '*/*'.
        Returns:
            None
        """
        async with self.__limiter:
            await self.__get_and_save_site_data(session=session, url=url, accept=accept)

    def __checking(self, 
                   html: str) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏–π –ª–∏ —Å–∞–π—Ç –∏–ª–∏ –Ω–µ—Ç
        Args:
            html (str): –∫–æ–¥ html —Å—Ç—Ä–∞–Ω–∏—Ü—ã

        Returns:
            bool: True- –ø–æ–¥—Ö–æ–¥–∏—Ç, False - –Ω–µ–ø–æ–¥—Ö–æ–¥–∏—Ç
        """
        try:
            soup = BeautifulSoup(html, 'lxml')
        except TypeError:
            print(html, "—Ç–∏–ø None")
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        title_tag = soup.find('title')
        if title_tag is not None:
            title_text = title_tag.text.lower()
            # –ë–æ–ª–µ–µ –º—è–≥–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ - –∏—â–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            if '–ø—Ä–∞–π—Å' in title_text and '23met' in title_text:
                # –ï—Å–ª–∏ –µ—Å—Ç—å —Ñ–∏–ª—å—Ç—Ä—ã –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º, –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ç–∞–±–ª–∏—Ü
                if self.__filter_keywords:
                    return self.__check_table_content(soup)
                return True
        return False
    
    def __check_table_content(self, soup: BeautifulSoup) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ç–∞–±–ª–∏—Ü –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        Args:
            soup (BeautifulSoup): –æ–±—ä–µ–∫—Ç BeautifulSoup —Å—Ç—Ä–∞–Ω–∏—Ü—ã

        Returns:
            bool: True- –ø–æ–¥—Ö–æ–¥–∏—Ç –ø–æ–¥ —Ñ–∏–ª—å—Ç—Ä, False - –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç
        """
        tables = soup.find_all('table', 'tablesorter')
        
        if not tables:
            return False
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç –∏–∑ —Ç–∞–±–ª–∏—Ü
        table_texts = []
        for table in tables:
            table_text = table.get_text().lower()
            table_texts.append(table_text)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç
        all_text = ' '.join(table_texts)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        if self.__filter_mode == "any":
            # –õ—é–±–æ–µ –∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–æ–ª–∂–Ω–æ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤–æ–≤–∞—Ç—å
            return any(keyword.lower() in all_text for keyword in self.__filter_keywords)
        elif self.__filter_mode == "all":
            # –í—Å–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–æ–ª–∂–Ω—ã –ø—Ä–∏—Å—É—Ç—Å—Ç–≤–æ–≤–∞—Ç—å
            return all(keyword.lower() in all_text for keyword in self.__filter_keywords)
        
        return False
    
    def __check_row_content(self, td_elements: list) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç—Ä–æ–∫–∏ —Ç–∞–±–ª–∏—Ü—ã –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        Args:
            td_elements (list): –°–ø–∏—Å–æ–∫ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ td –≤ —Å—Ç—Ä–æ–∫–µ

        Returns:
            bool: True- —Å—Ç—Ä–æ–∫–∞ –ø–æ–¥—Ö–æ–¥–∏—Ç –ø–æ–¥ —Ñ–∏–ª—å—Ç—Ä, False - –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç
        """
        if not self.__filter_keywords:
            return True  # –ï—Å–ª–∏ —Ñ–∏–ª—å—Ç—Ä –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –≤—Å–µ —Å—Ç—Ä–æ–∫–∏ –ø—Ä–æ—Ö–æ–¥—è—Ç
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç –∏–∑ —è—á–µ–µ–∫ —Å—Ç—Ä–æ–∫–∏
        row_text = ' '.join([td.text.strip() for td in td_elements]).lower()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        if self.__filter_mode == "any":
            # –õ—é–±–æ–µ –∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–æ–ª–∂–Ω–æ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤–æ–≤–∞—Ç—å
            return any(keyword.lower() in row_text for keyword in self.__filter_keywords)
        elif self.__filter_mode == "all":
            # –í—Å–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–æ–ª–∂–Ω—ã –ø—Ä–∏—Å—É—Ç—Å—Ç–≤–æ–≤–∞—Ç—å
            return all(keyword.lower() in row_text for keyword in self.__filter_keywords)
        
        return True
    
    def __extract_company_info(self, soup: BeautifulSoup) -> dict:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–º–ø–∞–Ω–∏–∏ –∏ –≥–æ—Ä–æ–¥–µ –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        Args:
            soup (BeautifulSoup): –æ–±—ä–µ–∫—Ç BeautifulSoup —Å—Ç—Ä–∞–Ω–∏—Ü—ã

        Returns:
            dict: —Å–ª–æ–≤–∞—Ä—å —Å –∫–ª—é—á–∞–º–∏ 'company' –∏ 'city'
        """
        company_info = {'company': None, 'city': None}
        
        try:
            # –°–ø–æ—Å–æ–± 1: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑ title
            title_tag = soup.find('title')
            if title_tag:
                title_text = title_tag.text.strip()
                # –§–æ—Ä–º–∞—Ç: "–£–¢–ö-–°—Ç–∞–ª—å | –ú–æ—Å–∫–≤–∞ | –ø—Ä–∞–π—Å-–ª–∏—Å—Ç ‚Äî 23MET.ru"
                if '|' in title_text:
                    parts = title_text.split('|')
                    if len(parts) >= 2:
                        company_info['company'] = parts[0].strip()
                        city_part = parts[1].strip()
                        # –£–±–∏—Ä–∞–µ–º "–ø—Ä–∞–π—Å-–ª–∏—Å—Ç" –µ—Å–ª–∏ –µ—Å—Ç—å
                        if '–ø—Ä–∞–π—Å-–ª–∏—Å—Ç' in city_part:
                            city_part = city_part.split('–ø—Ä–∞–π—Å-–ª–∏—Å—Ç')[0].strip()
                        company_info['city'] = city_part
            
            # –°–ø–æ—Å–æ–± 2: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑ meta description
            if not company_info['company'] or not company_info['city']:
                meta_desc = soup.find('meta', {'name': 'description'})
                if meta_desc and meta_desc.get('content'):
                    desc_text = meta_desc.get('content')
                    # –§–æ—Ä–º–∞—Ç: "–ø—Ä–∞–π—Å-–ª–∏—Å—Ç –£–¢–ö-–°—Ç–∞–ª—å –ú–æ—Å–∫–≤–∞"
                    if '–ø—Ä–∞–π—Å-–ª–∏—Å—Ç' in desc_text:
                        parts = desc_text.replace('–ø—Ä–∞–π—Å-–ª–∏—Å—Ç', '').strip().split()
                        if len(parts) >= 2:
                            company_info['company'] = parts[0]
                            company_info['city'] = parts[1]
            
            # –°–ø–æ—Å–æ–± 3: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑ h1 –∑–∞–≥–æ–ª–æ–≤–∫–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å)
            if not company_info['company'] or not company_info['city']:
                h1_tag = soup.find('h1', class_='h1_plist')
                if h1_tag:
                    h1_text = h1_tag.text.strip()
                    # –§–æ—Ä–º–∞—Ç: "–£–¢–ö-–°—Ç–∞–ª—å | –ú–æ—Å–∫–≤–∞"
                    if '|' in h1_text:
                        parts = h1_text.split('|')
                        if len(parts) >= 2:
                            company_info['company'] = parts[0].strip()
                            company_info['city'] = parts[1].strip()
            
            # –°–ø–æ—Å–æ–± 4: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑ div —Å id="plist-page-title"
            if not company_info['company'] or not company_info['city']:
                title_div = soup.find('div', id='plist-page-title')
                if title_div:
                    h1_in_div = title_div.find('h1', class_='h1_plist')
                    if h1_in_div:
                        h1_text = h1_in_div.text.strip()
                        if '|' in h1_text:
                            parts = h1_text.split('|')
                            if len(parts) >= 2:
                                company_info['company'] = parts[0].strip()
                                company_info['city'] = parts[1].strip()
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–æ–º–ø–∞–Ω–∏–∏: {e}")
        
        # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        if not company_info['company']:
            company_info['company'] = '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∫–æ–º–ø–∞–Ω–∏—è'
        if not company_info['city']:
            company_info['city'] = '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –≥–æ—Ä–æ–¥'
        
        return company_info
    
    def set_filter(self, keywords: list, mode: str = "any") -> None:
        """
        –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç —Ñ–∏–ª—å—Ç—Ä –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
        Args:
            keywords (list): –°–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
            mode (str): –†–µ–∂–∏–º —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: "any" (–ª—é–±–æ–µ —Å–ª–æ–≤–æ) –∏–ª–∏ "all" (–≤—Å–µ —Å–ª–æ–≤–∞)
        """
        self.__filter_keywords = keywords
        self.__filter_mode = mode
        print(f"–£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω —Ñ–∏–ª—å—Ç—Ä: {keywords} (—Ä–µ–∂–∏–º: {mode})")

    async def save_data(self,
                        accept: str= '*/*',
                        with_update_sites_info: bool= False,
                        num: int= 100,
                        start: int= 0,
                        stop: int= 100) -> None:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ —Å–æ –≤—Å–µ—Ö —Å–∞–π—Ç–æ–≤ (–≤—ã–¥–∞–Ω–Ω—ã—Ö Google-–ø–æ–∏—Å–∫–æ–º) –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏—Ö –≤ —Ñ–∞–π–ª—ã 
        Args:
            accept (str, optional): —Ç–∏–ø—ã —Ñ–∞–π–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –∫–ª–∏–µ–Ω—Ç –º–æ–∂–µ—Ç –ø—Ä–∏–Ω—è—Ç—å (–æ—Ç–æ–±—Ä–∞–∂–∞–µ—Ç—Å—è –±—Ä–∞—É–∑–µ—Ä–æ–º –≤ header-e). Defaults to '*/*'
            with_update_sites_info (bool, optional): –ü—Ä–æ—Å—Ç–æ –æ–±–Ω–æ–≤–∏—Ç—å –≤—Å–µ —Å–∞–π—Ç—ã –∏–ª–∏ –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å–ø–∞—Ä—Å–∏—Ç—å –∏ Google-–ø–æ–∏—Å–∫?. Defaults to False.
            num (int, optional): –ö–æ–ª-–≤–æ —Å–∞–π—Ç–æ–≤ –æ—Ç–æ–±—Ä–∞–∂–∞–µ–º–æ–µ Googl-–æ–º –Ω–∞ –æ–¥–Ω–æ–π –µ–µ html —Å—Ç—Ä–∞–Ω–∏—Ü–µ . Defaults to 100.
            start (int, optional): –° –∫–∞–∫–æ–≥–æ —Å–∞–π—Ç–∞ –Ω–∞—á–∞—Ç—å –æ—Ç–æ–±—Ä–∞–∂–∞—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ Google –ø–æ–∏—Å–∫–µ. Defaults to 0.
            stop (int, optional): –ù–∞ –∫–∞–∫–æ–º —Å–∞–π—Ç–µ –∑–∞–∫–æ–Ω—á–∏—Ç—å –æ—Ç–æ–±—Ä–∞–∂–∞—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ Google –ø–æ–∏—Å–∫–µ. Defaults to 100.
        Returns:
            None
        """
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞—Å—Ç–æ–º–Ω—ã–µ URL –µ—Å–ª–∏ –æ–Ω–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã, –∏–Ω–∞—á–µ Google –ø–æ–∏—Å–∫
        if self.__custom_urls:
            urls = self.__custom_urls
            print(f"üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞—Å—Ç–æ–º–Ω—ã–π —Å–ø–∏—Å–æ–∫ –∏–∑ {len(urls)} —Å–∞–π—Ç–æ–≤")
        else:
            google_searcher = GoogleParser(query_for_browser= 'site:23met.ru –ø—Ä–∞–π—Å-–ª–∏—Å—Ç')
            if with_update_sites_info:
                await google_searcher.run(num= num, 
                                          start= start, 
                                          stop= stop)
            else:
                await google_searcher.parsing()
            
            urls = google_searcher.get_urls()
            
        async with aiohttp.ClientSession() as session:
            tasks = []
            for url in urls:
                task = asyncio.create_task(self.__process_single_url_with_limiter(session= session, url= url, accept= accept))
                tasks.append(task)
            await asyncio.gather(*tasks)
    
    async def __get_one_site_unique_columns_name(self, 
                                                 file_path: str) -> Union[None, set]:
        """
        –ò—â–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Å–∞–π—Ç–∞(file_path)
        Args:
            file_path (str): –∞–±—Å–æ–ª—é—Ç–Ω—ã–π –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É

        Returns:
            Union[None, set]: –ï—Å–ª–∏ None, —Ç–æ –Ω–µ –Ω–∞—à–ª–æ—Å—å —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –Ω–∞–∑–≤–∞–Ω–∏–π –∫–æ–ª–æ–Ω–æ–∫
        """

        html = await self.get_file(file_path)
        unique_column_names = set()
        if self.__checking(html):
            soup = BeautifulSoup(html, 'lxml')
            tables = soup.find_all('table', 'tablesorter')
            for table in tables:
                table: BeautifulSoup
                columns_name = table.find('thead').find_all('th')
                for column_name in columns_name:
                    unique_column_names.add(column_name.text)
            return unique_column_names
        else:
            return None
        
    async def __get_all_unique_columns_name(self) -> list:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –Ω–∞–∑–≤–∞–Ω–∏–π –∫–æ–ª–æ–Ω–æ–∫ —Å–æ –≤—Å–µ—Ö —Å–∞–π—Ç–æ–≤
        Returns:
            list: —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –Ω–∞–∑–≤–∞–Ω–∏–π –∫–æ–ª–æ–Ω–æ–∫ —Å–æ –≤—Å–µ—Ö —Å–∞–π—Ç–æ–≤
        """
        tasks = []
        if not self.__file_paths:
            print("–ù–µ –±—ã–ª –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω self.__file_paths. –°–æ–∑–¥–∞—é –µ–≥–æ —Å–∞–º")
            file_names = os.listdir(self._dir_path)
            self.__file_paths = [os.path.join(self._dir_path, file_name) for file_name in file_names]
        
        for file_path in self.__file_paths:
            tasks.append(asyncio.create_task(self.__get_one_site_unique_columns_name(file_path)))
        results = [result for result in await asyncio.gather(*tasks) if result]
        unique_columns = list({item for result in results  for item in result})
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–æ–º–ø–∞–Ω–∏–∏ –∏ –≥–æ—Ä–æ–¥–µ
        if '–ö–æ–º–ø–∞–Ω–∏—è' not in unique_columns:
            unique_columns.append('–ö–æ–º–ø–∞–Ω–∏—è')
        if '–ì–æ—Ä–æ–¥' not in unique_columns:
            unique_columns.append('–ì–æ—Ä–æ–¥')
        
        return unique_columns

    
    async def _parsing_one_site(self, 
                                file_path: str) -> Union[None, dict]:
        """
        –ü–∞—Ä—Å–∏—Ç 1 —Å–∞–π—Ç(–≤ file_path)
        Args:
            file_path (str): –∞–±—Å–æ–ª—é—Ç–Ω—ã–π –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É

        Returns:
            Union[None, dict]: –ï—Å–ª–∏ None, —Ç–æ –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–ø–∞—Ä—Å–∏—Ç—å —Å–∞–π—Ç. 
                              dict - –¥–∞–Ω–Ω—ã–µ —Å –∫–ª—é—á–∞–º–∏: 'data' (–¥–∞–Ω–Ω—ã–µ), 'stats' (—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞)
        """
        html = await self.get_file(file_path)
        data = dict()
        stats = {'total_rows': 0, 'filtered_rows': 0}
        
        if not self.__unique_columns_name:
            print("–ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è self.__unique_columns_name –Ω–µ –±—ã–ª–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é –µ–µ!")
            self.__unique_columns_name = await self.__get_all_unique_columns_name()
        for column_name in self.__unique_columns_name:
            data[column_name] = []

        if self.__checking(html):
            soup = BeautifulSoup(html, 'lxml')
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–º–ø–∞–Ω–∏–∏ –∏ –≥–æ—Ä–æ–¥–µ
            company_info = self.__extract_company_info(soup)
            
            tables = soup.find_all('table', 'tablesorter')

            for table in tables:
                table: BeautifulSoup
                columns_name = [column.text for column in table.find('thead').find_all('th')]
                trS_in_tbody = table.find('tbody').find_all('tr')
                for tr_in_tbody in trS_in_tbody:
                    tdS_in_tbody = tr_in_tbody.find_all('td')
                    stats['total_rows'] += 1
                    
                    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Å—Ç—Ä–æ–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
                    if self.__filter_keywords and not self.__check_row_content(tdS_in_tbody):
                        continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —ç—Ç—É —Å—Ç—Ä–æ–∫—É, –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç –ø–æ–¥ —Ñ–∏–ª—å—Ç—Ä
                    
                    stats['filtered_rows'] += 1
                    
                    for column_name, td_in_tbody in zip(columns_name, tdS_in_tbody):
                        if td_in_tbody.text == '':
                            data[column_name].append(None)
                        else:
                            data[column_name].append(td_in_tbody.text)
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–º–ø–∞–Ω–∏–∏ –∏ –≥–æ—Ä–æ–¥–µ –∫ –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–µ
                    data['–ö–æ–º–ø–∞–Ω–∏—è'].append(company_info['company'])
                    data['–ì–æ—Ä–æ–¥'].append(company_info['city'])
                    
                    for unique_column_name in self.__unique_columns_name:
                        if unique_column_name not in columns_name and unique_column_name not in ['–ö–æ–º–ø–∞–Ω–∏—è', '–ì–æ—Ä–æ–¥']:
                            data[unique_column_name].append(None)
            
            return {'data': data, 'stats': stats}
        
        else:
            return None
        

    def __delete_intermediate_data(self) -> None:
        """
        –£–¥–∞–ª–µ–Ω–∏–µ –Ω–µ–Ω—É–∂–Ω—ã—Ö/–ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤

        Returns:
            None
        """
        for file_path in self.__file_paths:
            os.remove(file_path)


    async def parsing(self, 
                      with_save_result: bool= True) -> pd.DataFrame:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Å–∞–π—Ç–æ–≤.
        Args:
            with_save_result (bool, optional): –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ csv —Ñ–∞–π–ª?. Defaults to True.

        Returns:
            pd.DataFrame: DataFrame - –≤ –∫–æ—Ç–æ—Ä–æ–º —Ö—Ä–∞–Ω–∏—Ç—å—Å—è –≤—Å–µ —Å–ø–∞—Ä—â–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        """
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ñ–∏–ª—å—Ç—Ä–∞—Ö
        if self.__filter_keywords:
            print(f"üîç –ü—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è —Ñ–∏–ª—å—Ç—Ä: {self.__filter_keywords} (—Ä–µ–∂–∏–º: {self.__filter_mode})")
            print("üìã –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ –¥–≤—É—Ö —É—Ä–æ–≤–Ω—è—Ö:")
            print("   1. –£—Ä–æ–≤–µ–Ω—å —Å–∞–π—Ç–æ–≤ - –ø—Ä–æ–≤–µ—Ä—è–µ—Ç—Å—è —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –≤—Å–µ—Ö —Ç–∞–±–ª–∏—Ü")
            print("   2. –£—Ä–æ–≤–µ–Ω—å —Å—Ç—Ä–æ–∫ - –ø—Ä–æ–≤–µ—Ä—è–µ—Ç—Å—è –∫–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ –≤ —Ç–∞–±–ª–∏—Ü–∞—Ö")
        else:
            print("üîç –§–∏–ª—å—Ç—Ä –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω - –ø–∞—Ä—Å—è—Ç—Å—è –≤—Å–µ —Å–∞–π—Ç—ã –∏ —Å—Ç—Ä–æ–∫–∏")
        
        file_names = os.listdir(self._dir_path)
        self.__file_paths = [os.path.join(self._dir_path, file_name) for file_name in file_names]
        self.__unique_columns_name = await self.__get_all_unique_columns_name()
        data = dict()
        for column_name in self.__unique_columns_name:
            data[column_name] = []

        main_df = pd.DataFrame(data)
        tasks = []
        for file_path in self.__file_paths:
            tasks.append(asyncio.create_task(self._parsing_one_site(file_path)))
        results = await asyncio.gather(*tasks)
        
        sites_without_needing_data= []
        df_s = dict()
        total_rows_all_sites = 0
        filtered_rows_all_sites = 0
        
        for index, result in enumerate(results):
            if not result:
                sites_without_needing_data.append(self.__file_paths[index])
            else:
                try:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                    site_data = result['data']
                    site_stats = result['stats']
                    
                    total_rows_all_sites += site_stats['total_rows']
                    filtered_rows_all_sites += site_stats['filtered_rows']
                    
                    df_s[self.__file_paths[index]] = pd.DataFrame(data= site_data)
                except ValueError:
                    print("–ù–µ –≤—Å–µ –º–∞—Å–∏–≤—ã –æ–¥–Ω–æ–π –¥–ª–∏–Ω–Ω—ã —Ç—É—Ç:", self.__file_paths[index])

        if sites_without_needing_data:
            print("‚ùå –≠—Ç–∏ —Å–∞–π—Ç—ã –Ω–µ –ø–æ–¥—Ö–æ–¥—è—Ç –ø–æ–¥ —à–∞–±–ª–æ–Ω –ø–∞—Ä—Å–∏–Ω–≥–∞:", sites_without_needing_data)
        
        if self.__filter_keywords:
            filtered_sites_count = len(sites_without_needing_data)
            total_sites_count = len(self.__file_paths)
            print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Å–∞–π—Ç–æ–≤: {total_sites_count - filtered_sites_count}/{total_sites_count} —Å–∞–π—Ç–æ–≤ –ø—Ä–æ—à–ª–∏ —Ñ–∏–ª—å—Ç—Ä")
            print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Å—Ç—Ä–æ–∫: {filtered_rows_all_sites}/{total_rows_all_sites} —Å—Ç—Ä–æ–∫ –ø—Ä–æ—à–ª–∏ —Ñ–∏–ª—å—Ç—Ä")
        
        if df_s:
            main_df = pd.concat(list(df_s.values()), ignore_index=True)
            main_df = main_df.sort_values(by= '–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ', ignore_index=True)
            if with_save_result:
                main_df.to_csv(os.path.join(self._dir_path, 'result.csv'))
                print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(main_df)} –∑–∞–ø–∏—Å–µ–π –≤ result.csv")
        else:
            print("‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è - –≤—Å–µ —Å–∞–π—Ç—ã –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω—ã")
            if with_save_result:
                # –°–æ–∑–¥–∞–µ–º –ø—É—Å—Ç–æ–π —Ñ–∞–π–ª —Å –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏
                empty_df = pd.DataFrame(columns=self.__unique_columns_name)
                empty_df.to_csv(os.path.join(self._dir_path, 'result.csv'))
                print("üìÑ –°–æ–∑–¥–∞–Ω –ø—É—Å—Ç–æ–π —Ñ–∞–π–ª result.csv")
            main_df = pd.DataFrame()
        return main_df
        

    async def run(self,
                  accept: str= '*/*',
                  num: int= 100,
                  start: int= 0,
                  stop: int= 100,
                  with_update_sites_info: bool= False,
                  with_save_result: bool= True,
                  with_remove_intermediate_data: bool= False) -> None:
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥, –ø–æ—Å–ª–µ –∑–∞–ø—É—Å–∫–∞ –∫–æ—Ç–æ—Ä–æ–≥–æ –≤—ã–ø–æ–ª–Ω—è—Ç—Å—è –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –º–µ—Ç–æ–¥—ã –≤ –Ω—É–∂–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –∞ –∏–º–µ–Ω–Ω–æ:
        1) –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ html —Å—Ç—Ä–∞–Ω–∏—Ü –≤ —Ñ–∞–π–ª—ã
        2) –ó–∞–±–æ—Ä –Ω—É–∂–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ —ç—Ç–∏—Ö —Ñ–∞–π–ª–æ–≤
        3) –ü—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ —É–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤

        Args:
            accept (str, optional): —Ç–∏–ø—ã —Ñ–∞–π–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –∫–ª–∏–µ–Ω—Ç –º–æ–∂–µ—Ç –ø—Ä–∏–Ω—è—Ç—å (–æ—Ç–æ–±—Ä–∞–∂–∞–µ—Ç—Å—è –±—Ä–∞—É–∑–µ—Ä–æ–º –≤ header-e). Defaults to '*/*'
            with_update_sites_info (bool, optional): –ü—Ä–æ—Å—Ç–æ –æ–±–Ω–æ–≤–∏—Ç—å –≤—Å–µ —Å–∞–π—Ç—ã –∏–ª–∏ –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å–ø–∞—Ä—Å–∏—Ç—å –∏ Google-–ø–æ–∏—Å–∫?. Defaults to False.
            num (int, optional): –ö–æ–ª-–≤–æ —Å–∞–π—Ç–æ–≤ –æ—Ç–æ–±—Ä–∞–∂–∞–µ–º–æ–µ Googl-–æ–º –Ω–∞ –æ–¥–Ω–æ–π –µ–µ html —Å—Ç—Ä–∞–Ω–∏—Ü–µ . Defaults to 100.
            start (int, optional): –° –∫–∞–∫–æ–≥–æ —Å–∞–π—Ç–∞ –Ω–∞—á–∞—Ç—å –æ—Ç–æ–±—Ä–∞–∂–∞—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ Google –ø–æ–∏—Å–∫–µ. Defaults to 0.
            stop (int, optional): –ù–∞ –∫–∞–∫–æ–º —Å–∞–π—Ç–µ –∑–∞–∫–æ–Ω—á–∏—Ç—å –æ—Ç–æ–±—Ä–∞–∂–∞—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ Google –ø–æ–∏—Å–∫–µ. Defaults to 100.
            with_save_result (bool, optional): –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Ñ–∞–π–ª?. Defaults to True.
            with_remove_intermediate_data (bool, optional): –£–¥–∞–ª–∏—Ç—å –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ñ–∞–π–ª—ã?. Defaults to False.
        
        Returns:
            None
        """
        
        print("–ù–∞—á–∏–Ω–∞—é –ø—Ä–æ—Ü–µ—Å—Å —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö —Å —Å–∞–π—Ç–∞")
        await self.save_data(accept= accept,
                             with_update_sites_info= with_update_sites_info,
                             num= num,
                             start= start,
                             stop= stop)
        
        print("–ù–∞—á–∏–Ω–∞—é –ø—Ä–æ—Ü–µ—Å—Å –∑–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö —Å–æ —Å–∫–∞—á–µ–Ω–Ω—ã—Ö —Å–∞–π—Ç–æ–≤")
        await self.parsing(with_save_result= with_save_result)

        if with_remove_intermediate_data:
            print("–£–¥–∞–ª—è—é –≤—Å–µ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
            self.__delete_intermediate_data()     
